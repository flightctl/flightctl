# Troubleshooting

## Verifying the effective device specification received by the device agent

When viewing a device resource using the command

```console
flightctl get device/${device_name} -o yaml
```

the output contains the device specification as specified by the user or the fleet controller based on the fleet's device template. That specification may contain references to
configuration or secrets stored on external systems, such a Git or a Kubernetes cluster.

Only when the device agent queries the service, the service replaces these references with the actual configuration and secret data. While this better protects potentially
sensitive data, it also makes troubleshooting faulty configurations hard.

Users with the `GetRenderedDevice` permission can run the following command to view the effective configuration as rendered by the service to the device agent:

```console
flightctl get device/${device_name} -o yaml --rendered
```

## Generating and downloading an SOS report

SOS reports are a collection of system information that can be used to debug issues with the device agent. The SOS report is generated by the `sos report` command, which is
included in the device image.
To generate an SOS report, run the following console command on the device:

```console
flightctl console device/${device_name} -- sos report --batch --quiet
```

The output is a tarball named `sosreport-$hostname-$timestamp-$random-sequence.tar.xz`, whereby `$hostname` is the device's hostname and `$timestamp` is the current date and time.
The tarball contains system information, logs, and configuration files that can be used to debug issues with the device agent.

Output example:

```console
sos report (version 4.8.1)


Your sos report has been generated and saved in:
       /var/tmp/sosreport-localhost-2025-04-28-svjuich.tar.xz

 Size  11.83MiB
 Owner root
 sha256        918563a260c5d6a069e178e9ddce643b3b93ad2a4fdca9a3e431b63b2e82041d

Please send this file to your support representative.
```

To download the SOS report, run the following command on the device:

```console
flightctl console device/${device_name} -- cat /var/tmp/sosreport-localhost-2025-04-28-svjuich.tar.xz > sosreport.tar.xz 
```

## Resource-Aware Update Issues

### OS Update Stuck in "Preparing" State

When an OS update gets stuck in the preparing state, this usually indicates issues with image prefetching or resource constraints.

#### Symptoms

- Device status shows `status.updated.status: OutOfDate` for extended periods
- Update conditions show `Preparing` or `PrefetchNotReady` states
- Large OS images fail to download completely

#### Diagnosis

Check current resource status:

```console
flightctl get device/${device_name} -o yaml | grep -A 20 "resources:"
```

Monitor prefetch progress on the device:

```console
flightctl console device/${device_name} -- journalctl -u flightctl-agent | grep -i prefetch
```

Check available disk space:

```console
flightctl console device/${device_name} -- df -h
```

Check for container storage issues:

```console
flightctl console device/${device_name} -- podman system df
```

#### Resolution

1. **Free up disk space** if storage is critically low:

   ```console
   flightctl console device/${device_name} -- podman system prune -a
   flightctl console device/${device_name} -- journalctl --vacuum-time=7d
   ```

2. **Restart the agent** to retry failed downloads:

    ```console
    flightctl console device/${device_name} -- systemctl restart flightctl-agent
   ```

3. **Check network connectivity** and retry:

    ```console
    flightctl console device/${device_name} -- ping -c 3 registry.example.com
    ```

### Resource Alerts Not Resolving

#### Symptoms

- Device shows `Critical` or `Warning` resource status
- Resource usage has been reduced but alerts persist
- Updates are blocked due to resource alerts

#### Diagnosis

Check current resource usage vs. configured thresholds:

```console
flightctl get device/${device_name} -o yaml | grep -A 50 "resources:"
```

Monitor real-time usage on device:

```console
flightctl console device/${device_name} -- watch "df -h && echo '---' && free -h && echo '---' && top -bn1 | head -5"
```

#### Resolution

Resource monitors use duration-based thresholds, so alerts may persist briefly after usage decreases:

1. **Wait for the duration period** specified in your alert rules
2. **Verify thresholds are appropriate** for your device's normal usage patterns
3. **Restart the agent** if alerts seem stuck:

   ```console
   flightctl console device/${device_name} -- systemctl restart flightctl-agent
   ```

### Large OS Image Download Failures

#### Symptoms

- Timeouts during OS image layer downloads
- Repeated retry attempts for the same image layers
- "Layer size exceeds available storage" errors

#### Diagnosis

Check agent logs for layer-specific errors:

```console
flightctl console device/${device_name} -- journalctl -u flightctl-agent | grep -i -E "(layer|timeout|size)"
```

Inspect image layers manually (if accessible):

```console
flightctl console device/${device_name} -- podman manifest inspect registry.example.com/os-image:latest
```

#### Resolution

1. **Increase available storage**:

   ```console
   flightctl console device/${device_name} -- podman system prune -a --volumes
   flightctl console device/${device_name} -- docker system prune -a  # if using Docker
   ```

2. **Adjust resource thresholds** to allow more storage usage during updates:

   ```yaml
   resources:
   - monitorType: Disk
     path: /
     alertRules:
     - severity: Critical
       percentage: 85  # Was 75, increased to allow more space for downloads
   ```

3. **Use scheduled downloads** during low-usage periods:

   ```yaml
   updatePolicy:
     downloadSchedule:
       at: "0 2 * * *"  # Download at 2 AM when usage is typically low
   ```

### Prefetch Status Shows "Buffer Full"

#### Symptoms

- Prefetch operations fail with "buffer full" messages
- Multiple large images queued simultaneously
- Agent logs show prefetch scheduling failures

#### Diagnosis

Check the number of concurrent prefetch operations:

```console
flightctl console device/${device_name} -- journalctl -u flightctl-agent | grep -i "prefetch.*queue\|buffer"
```

#### Resolution

1. **Wait for current downloads** to complete before queuing more
2. **Stagger updates** across devices in a fleet to reduce load
3. **Monitor and adjust** the agent's pull timeout configuration

### Memory Pressure During Updates

#### Symptoms

- Device becomes unresponsive during OS updates
- Out of memory errors in system logs
- Applications killed by OOM killer during updates

#### Diagnosis

Check memory usage and OOM events:

```console
flightctl console device/${device_name} -- dmesg | grep -i "killed\|oom"
flightctl console device/${device_name} -- free -h
```

Monitor memory during update process:

```console
flightctl console device/${device_name} -- watch "free -h && echo '---' && ps aux --sort=-%mem | head -10"
```

#### Resolution

1. **Configure memory monitoring** with appropriate thresholds:

   ```yaml
   resources:
   - monitorType: Memory
     samplingInterval: 30s
     alertRules:
     - severity: Warning
       duration: 5m
       percentage: 75
     - severity: Critical
       duration: 2m
       percentage: 85
   ```

2. **Schedule updates during low-memory periods**:

   ```yaml
   updatePolicy:
     updateSchedule:
       at: "0 3 * * 0"  # Sunday 3 AM when fewer processes are running
   ```

3. **Stop non-essential services** before major updates:

   ```yaml
   # Use device lifecycle hooks
   # /etc/flightctl/hooks.d/beforeupdating/stop-services.yaml
   - run: /usr/bin/systemctl stop my-memory-intensive-service
     timeout: 30s
   ```

### Update Policy Conflicts

#### Symptoms

- Updates scheduled but never execute
- Policy evaluation errors in agent logs
- Download policy satisfied but update policy is not

#### Diagnosis

Check update policy configuration and current time evaluation:

```console
flightctl get device/${device_name} -o yaml | grep -A 20 "updatePolicy:"
flightctl console device/${device_name} -- date
flightctl console device/${device_name} -- journalctl -u flightctl-agent | grep -i policy
```

#### Resolution

1. **Verify time zone configuration**:

   ```console
   flightctl console device/${device_name} -- timedatectl status
   ```

2. **Test cron expressions** using online tools like [crontab.guru](https://crontab.guru/)

3. **Add grace duration** to account for timing variations:

   ```yaml
   updatePolicy:
     updateSchedule:
       at: "0 4 * * 0"
       startGraceDuration: "2h"  # Allow 2-hour window
   ```

### Debugging Resource Monitor Configuration

#### Verify Monitor Active Status

Check if resource monitors are properly configured and running:

```console
flightctl console device/${device_name} -- journalctl -u flightctl-agent | grep -i "resource\|monitor\|alert"
```

#### Test Resource Threshold Triggers

Temporarily create high resource usage to test alert triggers:

**Disk usage test** (use with caution):

```console
flightctl console device/${device_name} -- dd if=/dev/zero of=/tmp/testfile bs=1M count=100
# Monitor alerts, then clean up:
flightctl console device/${device_name} -- rm /tmp/testfile
```

**Memory usage test** (requires installing stress tool):

```console
# First install stress tool
flightctl console device/${device_name} -- dnf install stress --assumeyes

# Then run memory test (use caution with test size)
flightctl console device/${device_name} -- stress --vm 1 --vm-bytes 256M --timeout 30s
```

> [!WARNING]
> Use caution with memory stress tests. Start with smaller values (256M) and shorter durations (30s) to avoid system instability.

#### Validate Configuration Syntax

Ensure resource monitor configuration is valid:

```console
flightctl get device/${device_name} -o yaml | grep -A 50 "resources:" | flightctl apply --dry-run -f -
```
